{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-Qhy6Rcnfjg"
      },
      "source": [
        "# Word features in LLM exp 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7MjKk6EBWb"
      },
      "source": [
        "### preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpbupk3xoD9n"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade openai tiktoken openpyxl --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you use Google Colab, you can run the following commands to clone the repository and navigate to the `llm` directory:\n",
        "\n",
        "# !git clone https://github.com/hagi-hara/psychfeaturesllm.git\n",
        "# %cd psychfeaturesllm/llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# use your own API keys\n",
        "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai_api_key = \"\"\n",
        "# https://deepinfra.com/\n",
        "# deepinfra_api_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
        "deepinfra_api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg9MW7vgn_m1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import openai\n",
        "from openai import AsyncOpenAI\n",
        "from openai import OpenAI\n",
        "\n",
        "import os\n",
        "import time\n",
        "import statistics\n",
        "\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M4-9QjZ7tCW"
      },
      "outputs": [],
      "source": [
        "PROMPT_LIST_DF = pd.read_excel(\"./dataset/prompt_list.xlsx\", header=0)\n",
        "\n",
        "WORD_FEATURES_DF_EXP2 = pd.read_csv(\"./dataset/wordlist_exp2.csv\", header=0)\n",
        "\n",
        "WORD_FEATURES_DF_EXP2.arousal = WORD_FEATURES_DF_EXP2.arousal2\n",
        "WORD_FEATURES_DF_EXP2.valence = WORD_FEATURES_DF_EXP2.valence2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOJNIvfVdL7k"
      },
      "outputs": [],
      "source": [
        "async def fetch_api(client, messages, model_id, model_name, feature_type, prop, i):\n",
        "  '''Fetch API and save the response to a file.'''\n",
        "  completion = await client.chat.completions.create(\n",
        "    model= model_id,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        "    max_tokens=4096,\n",
        "    # max_tokens=100000,\n",
        "    messages=messages\n",
        "  )\n",
        "\n",
        "  with open(f\"results_exp2/{model_name}/{model_name}_{feature_type}_{prop}_{i}.txt\", mode=\"w\") as f:\n",
        "      f.write(completion.choices[0].message.content)\n",
        "\n",
        "  with open(f\"results_exp2/{model_name}/{model_name}_{feature_type}_{prop}_{i}.json\", 'w') as f:\n",
        "    json.dump(json.loads(completion.to_json()), f, indent=2)\n",
        "\n",
        "  return completion\n",
        "\n",
        "\n",
        "async def main(client, model_id, model_name, i,):\n",
        "  '''Main function to fetch API for each word feature type and property ratio.'''\n",
        "  fewshot_prompt = \"For reference, the mean ratings by human participants are listed below. Please rate each word in accordance with the ratings provided.\"\n",
        "  support_prompt = \"Please answer about all of the following words.\"\n",
        "  tasks = []\n",
        "  for feature_type in PROMPT_LIST_DF.dropna().feature_type:\n",
        "    # exclude socialness and imageability as they are missing\n",
        "    if feature_type == \"socialness\" or feature_type == \"imageability\":\n",
        "      continue\n",
        "    prompt = PROMPT_LIST_DF[PROMPT_LIST_DF.feature_type==feature_type].iat[0, 1]\n",
        "    for prop in [\"10\", \"20\", \"30\", \"40\", \"50\"]:\n",
        "      prop_ratio_and_index = f'prop{prop}.{i}'\n",
        "      training_index = WORD_FEATURES_DF_EXP2.loc[:, prop_ratio_and_index] == 'training'\n",
        "      test_index = WORD_FEATURES_DF_EXP2.loc[:, prop_ratio_and_index] == 'test'\n",
        "      test_words = WORD_FEATURES_DF_EXP2.loc[:, 'word_for_llm'][test_index].sample(frac=1, random_state=i)\n",
        "      word_str_for_prompt = \"\\n\".join(test_words.tolist())\n",
        "      training_words_and_scores = WORD_FEATURES_DF_EXP2.loc[:, ['word_for_llm',feature_type]][training_index].sample(frac=1, random_state=i)\n",
        "      # Round to the second decimal place from the third decimal place, and convert to a string.\n",
        "      training_words_and_scores_str = '\\n'.join([f\"{row['word_for_llm']}: {round(row[feature_type], 2):.2f}\" for _, row in training_words_and_scores.iterrows()])\n",
        "      all_prompt = prompt + \"\\n\\n\" + fewshot_prompt + \"\\n\\n\" + training_words_and_scores_str +  \"\\n\\n\" + support_prompt + \"\\n\\n\" + word_str_for_prompt\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a native English speaker.\"},\n",
        "        {\"role\": \"user\", \"content\": all_prompt}\n",
        "      ]\n",
        "      with open(f\"results_exp2/{model_name}/{model_name}_{feature_type}_{prop}_{i}_prompt.txt\", mode=\"w\") as f:\n",
        "        f.write(all_prompt)\n",
        "      tasks.append(fetch_api(client, messages, model_id, model_name, feature_type, prop, i))\n",
        "\n",
        "  results = await asyncio.gather(*tasks)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUHUxO1t6w4Z"
      },
      "outputs": [],
      "source": [
        "openapi_client = AsyncOpenAI(\n",
        "    api_key=openai_api_key,\n",
        "    base_url=\"https://api.openai.com/v1\"\n",
        ")\n",
        "deepinfra_client = AsyncOpenAI(\n",
        "    api_key=deepinfra_api_key,\n",
        "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
        ")\n",
        "\n",
        "model_list = [\n",
        "     {\n",
        "        \"client\": openapi_client,\n",
        "        \"model_id\": \"gpt-4o-2024-05-13\",\n",
        "        \"model_name\": \"gpt-4o-2024-05-13\"\n",
        "     },\n",
        "     {\n",
        "        \"client\": deepinfra_client,\n",
        "        \"model_id\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "        \"model_name\": \"meta-llama_Meta-Llama-3.1-70B-Instruct\"\n",
        "     },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "rhlZkMJjk-0-",
        "outputId": "dae0bb23-ff47-4de2-d1a4-056d9e6a292e"
      },
      "outputs": [],
      "source": [
        "num_of_iterations = 5\n",
        "for model in model_list:\n",
        "  for i in range(1, 1 + num_of_iterations):\n",
        "    client = model[\"client\"]\n",
        "    model_id = model[\"model_id\"]\n",
        "    model_name = model[\"model_name\"]\n",
        "\n",
        "    print(f\"model_name: {model_name}, iteration: {i}\")\n",
        "    os.makedirs(f\"results_exp2/{model_name}/\", exist_ok=True)\n",
        "    results = asyncio.run(main(client, model_id, model_name, i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRvk0udsMU0t"
      },
      "source": [
        "### Summarize the results of the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWmumIeYa9fl"
      },
      "outputs": [],
      "source": [
        "def get_feature_results(model_name, num_of_iterations):\n",
        "  '''get feature results from the saved files by parsing the output files.'''\n",
        "\n",
        "  word_score_df_list = []\n",
        "\n",
        "  for feature_type in PROMPT_LIST_DF.dropna().feature_type:\n",
        "    if feature_type == \"socialness\" or feature_type == \"imageability\":\n",
        "      continue\n",
        "    prompt = PROMPT_LIST_DF[PROMPT_LIST_DF.feature_type==feature_type].iat[0, 1]\n",
        "    for prop in [\"50\", \"40\", \"30\", \"20\", \"10\"]:\n",
        "      for i in range(1, 1 + num_of_iterations):\n",
        "\n",
        "        # load file\n",
        "        file_name = f\"results_exp2/{model_name}/{model_name}_{feature_type}_{prop}_{i}.json\"\n",
        "        with open(file_name, 'r') as f:\n",
        "          llm_output = json.load(f)['choices'][0]['message']['content']\n",
        "\n",
        "          # parse output\n",
        "          output_dict = {}\n",
        "          for output in  llm_output.splitlines():\n",
        "            try:\n",
        "              word, score = output.split(': ')\n",
        "            except Exception as e:\n",
        "              word = output\n",
        "              score = output\n",
        "            try:\n",
        "              # output_dict[word] = int(score)\n",
        "              output_dict[word] = float(score)\n",
        "            except:\n",
        "              output_dict[word] = score\n",
        "\n",
        "          # Order according to word_for_llm. For words not outputted, set the score to X. For training words, set it to T.\n",
        "          reordered_dict = {}\n",
        "          prop_ratio_and_index = f'prop{prop}.{i}'\n",
        "          training_index = WORD_FEATURES_DF_EXP2.loc[:, prop_ratio_and_index] == 'training'\n",
        "          training_words = WORD_FEATURES_DF_EXP2.loc[:, 'word_for_llm'][training_index]\n",
        "          for word in WORD_FEATURES_DF_EXP2.loc[:, 'word_for_llm'].unique():\n",
        "            if word in training_words.tolist():\n",
        "              reordered_dict[word] = \"T\"\n",
        "              try:\n",
        "                print(output_dict[word])\n",
        "                print(\"This is a training word\")\n",
        "              except:\n",
        "                pass\n",
        "              continue\n",
        "            try:\n",
        "              reordered_dict[word] = output_dict[word]\n",
        "            except:\n",
        "              reordered_dict[word] = \"X\"\n",
        "\n",
        "          word_score_df = pd.Series(reordered_dict, name=f\"{model_name}_{feature_type}_{prop}_{i}\")\n",
        "          word_score_df_list.append(word_score_df)\n",
        "\n",
        "  all_results_df = pd.concat(word_score_df_list, axis=1)\n",
        "\n",
        "  # Calculate the average of each word feature\n",
        "  for feature_type in PROMPT_LIST_DF.dropna().feature_type:\n",
        "    if feature_type == \"socialness\" or feature_type == \"imageability\":\n",
        "      continue\n",
        "    for prop in [\"50\", \"40\", \"30\", \"20\", \"10\"]:\n",
        "      tmp_df = all_results_df[[f\"{model_name}_{feature_type}_{prop}_{i}\" for i in range(1, 1 + num_of_iterations)]]\n",
        "      tmp_df.replace('N', pd.NA, inplace=True)\n",
        "      tmp_df.replace('X', pd.NA, inplace=True)\n",
        "      tmp_df.replace('T', pd.NA, inplace=True)\n",
        "      average = tmp_df.apply(pd.to_numeric, errors='coerce').mean(axis=1)\n",
        "      average.name = f\"{model_name}_{feature_type}_{prop}_average\"\n",
        "      all_results_df.loc[:, average.name] = average\n",
        "\n",
        "  all_results_df.index.name = 'word_used'\n",
        "  all_results_df = all_results_df.reset_index()\n",
        "\n",
        "  return all_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KgLnWY2ZQBb"
      },
      "source": [
        "### Save the results to csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-AAHzHFzgxHD",
        "outputId": "c23fb5a8-af19-46e1-9c43-57d3d8bf2d3a"
      },
      "outputs": [],
      "source": [
        "model_name_list = [model[\"model_name\"] for model in model_list]\n",
        "\n",
        "num_of_iterations = 5\n",
        "all_results_df_list = []\n",
        "for model_name in model_name_list:\n",
        "  all_results_df = get_feature_results(model_name, num_of_iterations)\n",
        "  all_results_df_list.append(all_results_df)\n",
        "  print(model_name)\n",
        "  \n",
        "concatenated_results_df = pd.concat([WORD_FEATURES_DF_EXP2] + all_results_df_list, axis=1)\n",
        "\n",
        "concatenated_results_df.to_csv('results_exp2/concatenated_results.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
